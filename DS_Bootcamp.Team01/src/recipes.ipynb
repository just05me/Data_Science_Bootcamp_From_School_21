{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOKVat17qUaM"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fmlSS0tdqLsG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "BSB125lerqiV",
        "outputId": "2944b11e-5a8a-40f1-e64f-1a0c95f46576"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>rating</th>\n",
              "      <th>calories</th>\n",
              "      <th>protein</th>\n",
              "      <th>fat</th>\n",
              "      <th>sodium</th>\n",
              "      <th>#cakeweek</th>\n",
              "      <th>#wasteless</th>\n",
              "      <th>22-minute meals</th>\n",
              "      <th>3-ingredient recipes</th>\n",
              "      <th>...</th>\n",
              "      <th>yellow squash</th>\n",
              "      <th>yogurt</th>\n",
              "      <th>yonkers</th>\n",
              "      <th>yuca</th>\n",
              "      <th>zucchini</th>\n",
              "      <th>cookbooks</th>\n",
              "      <th>leftovers</th>\n",
              "      <th>snack</th>\n",
              "      <th>snack week</th>\n",
              "      <th>turkey</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lentil, Apple, and Turkey Wrap</td>\n",
              "      <td>2.500</td>\n",
              "      <td>426.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>559.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Boudin Blanc Terrine with Red Onion Confit</td>\n",
              "      <td>4.375</td>\n",
              "      <td>403.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1439.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Potato and Fennel Soup Hodge</td>\n",
              "      <td>3.750</td>\n",
              "      <td>165.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mahi-Mahi in Tomato Olive Sauce</td>\n",
              "      <td>5.000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spinach Noodle Casserole</td>\n",
              "      <td>3.125</td>\n",
              "      <td>547.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>452.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 680 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         title  rating  calories  protein  \\\n",
              "0              Lentil, Apple, and Turkey Wrap    2.500     426.0     30.0   \n",
              "1  Boudin Blanc Terrine with Red Onion Confit    4.375     403.0     18.0   \n",
              "2                Potato and Fennel Soup Hodge    3.750     165.0      6.0   \n",
              "3             Mahi-Mahi in Tomato Olive Sauce    5.000       NaN      NaN   \n",
              "4                    Spinach Noodle Casserole    3.125     547.0     20.0   \n",
              "\n",
              "    fat  sodium  #cakeweek  #wasteless  22-minute meals  3-ingredient recipes  \\\n",
              "0   7.0   559.0        0.0         0.0              0.0                   0.0   \n",
              "1  23.0  1439.0        0.0         0.0              0.0                   0.0   \n",
              "2   7.0   165.0        0.0         0.0              0.0                   0.0   \n",
              "3   NaN     NaN        0.0         0.0              0.0                   0.0   \n",
              "4  32.0   452.0        0.0         0.0              0.0                   0.0   \n",
              "\n",
              "   ...  yellow squash  yogurt  yonkers  yuca  zucchini  cookbooks  leftovers  \\\n",
              "0  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "1  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "2  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "3  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "4  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "\n",
              "   snack  snack week  turkey  \n",
              "0    0.0         0.0     1.0  \n",
              "1    0.0         0.0     0.0  \n",
              "2    0.0         0.0     0.0  \n",
              "3    0.0         0.0     0.0  \n",
              "4    0.0         0.0     0.0  \n",
              "\n",
              "[5 rows x 680 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"epi_r.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh-VlBO4r41B",
        "outputId": "c8891200-b4d1-4523-858d-c1cb5c42215f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['title', 'rating', 'calories', 'protein', 'fat', 'sodium', '#cakeweek',\n",
              "       '#wasteless', '22-minute meals', '3-ingredient recipes',\n",
              "       ...\n",
              "       'yellow squash', 'yogurt', 'yonkers', 'yuca', 'zucchini', 'cookbooks',\n",
              "       'leftovers', 'snack', 'snack week', 'turkey'],\n",
              "      dtype='object', length=680)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0mOU9U6qbC0"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-xPRRzciqZBc"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path):\n",
        "    return pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPuqetyprcIO"
      },
      "source": [
        "I am filtering the relevant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U2ZcyUaZrdpz"
      },
      "outputs": [],
      "source": [
        "def filter_columns(data):\n",
        "   ingredient_columns = [col for col in data.columns if col not in ['title', 'calories', 'protein', 'fat', 'sodium', 'rating']]\n",
        "   return data[['rating'] + ingredient_columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LReyizTozxLF"
      },
      "source": [
        "And now handling the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5KTiHcbyzt4S"
      },
      "outputs": [],
      "source": [
        "def handle_missing_values(data):\n",
        "    data['rating'] = data['rating'].fillna(data['rating'].median())\n",
        "    ingredient_cols = data.columns[1:]\n",
        "    data = data[data[ingredient_cols].sum(axis=1) > 0]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MkreKZu0Eeh"
      },
      "source": [
        "and now I am preparing target variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gY-ZHyOi0CSY"
      },
      "outputs": [],
      "source": [
        "def prepare_target(data, classification=False):\n",
        "    if classification:\n",
        "        data['rating'] = data['rating'].apply(lambda x: 'bad' if x <= 1 else 'so-so' if x <= 3 else 'great')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1Vm_81L0RNl"
      },
      "source": [
        "I am going to save the cleaned dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I7iLkKei0Mc7"
      },
      "outputs": [],
      "source": [
        "def save_cleaned_data(data, output_path):\n",
        "    data.to_csv(output_path, index=False)\n",
        "    print(f\"Cleaned dataset saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHRb29940iB4"
      },
      "source": [
        "Main script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcSoJ5gD0k8z",
        "outputId": "d088901b-2668-4ee8-8a8f-9710e4b19281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset saved to recipes.csv\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Replace 'epicurious.csv' with your dataset file path\n",
        "    file_path = \"epi_r.csv\"\n",
        "    output_path = \"recipes.csv\"\n",
        "\n",
        "    # Load and process the dataset\n",
        "    data = load_dataset(file_path)\n",
        "    data = filter_columns(data)\n",
        "    data = handle_missing_values(data)\n",
        "    data = prepare_target(data, classification=True)\n",
        "    save_cleaned_data(data, output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "pi3bOF961UwS",
        "outputId": "75d83a19-2713-45e8-e60c-57702baf4da8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>#cakeweek</th>\n",
              "      <th>#wasteless</th>\n",
              "      <th>22-minute meals</th>\n",
              "      <th>3-ingredient recipes</th>\n",
              "      <th>30 days of groceries</th>\n",
              "      <th>advance prep required</th>\n",
              "      <th>alabama</th>\n",
              "      <th>alaska</th>\n",
              "      <th>alcoholic</th>\n",
              "      <th>...</th>\n",
              "      <th>yellow squash</th>\n",
              "      <th>yogurt</th>\n",
              "      <th>yonkers</th>\n",
              "      <th>yuca</th>\n",
              "      <th>zucchini</th>\n",
              "      <th>cookbooks</th>\n",
              "      <th>leftovers</th>\n",
              "      <th>snack</th>\n",
              "      <th>snack week</th>\n",
              "      <th>turkey</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>so-so</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>great</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>great</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 675 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  rating  #cakeweek  #wasteless  22-minute meals  3-ingredient recipes  \\\n",
              "0  so-so        0.0         0.0              0.0                   0.0   \n",
              "1  great        0.0         0.0              0.0                   0.0   \n",
              "2  great        0.0         0.0              0.0                   0.0   \n",
              "3  great        0.0         0.0              0.0                   0.0   \n",
              "4  great        0.0         0.0              0.0                   0.0   \n",
              "\n",
              "   30 days of groceries  advance prep required  alabama  alaska  alcoholic  \\\n",
              "0                   0.0                    0.0      0.0     0.0        0.0   \n",
              "1                   0.0                    0.0      0.0     0.0        0.0   \n",
              "2                   0.0                    0.0      0.0     0.0        0.0   \n",
              "3                   0.0                    0.0      0.0     0.0        0.0   \n",
              "4                   0.0                    0.0      0.0     0.0        0.0   \n",
              "\n",
              "   ...  yellow squash  yogurt  yonkers  yuca  zucchini  cookbooks  leftovers  \\\n",
              "0  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "1  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "2  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "3  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "4  ...            0.0     0.0      0.0   0.0       0.0        0.0        0.0   \n",
              "\n",
              "   snack  snack week  turkey  \n",
              "0    0.0         0.0     1.0  \n",
              "1    0.0         0.0     0.0  \n",
              "2    0.0         0.0     0.0  \n",
              "3    0.0         0.0     0.0  \n",
              "4    0.0         0.0     0.0  \n",
              "\n",
              "[5 rows x 675 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"recipes.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1-EVEe72QYl"
      },
      "source": [
        "So now I should separate the features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fpiCCpIT102R"
      },
      "outputs": [],
      "source": [
        "X = data.drop('rating', axis=1)\n",
        "y = data['rating']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1swU7WV2cWs"
      },
      "source": [
        "I am converting the target into categories if classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S7w3561v2aqp"
      },
      "outputs": [],
      "source": [
        "y = pd.to_numeric(y.str.replace('bad', '0').str.replace('so-so', '1').str.replace('great', '2'), errors='coerce')\n",
        "y = y.apply(lambda x: 'bad' if x <= 1 else 'so-so' if x <= 3 else 'great')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA72MKVR2wHT"
      },
      "source": [
        "Splitting into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1OsIndDJ2kod"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So6lq0gA3THy"
      },
      "source": [
        "## Now classification and regression models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X9VaaXo6po8"
      },
      "source": [
        "### Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C26pp_ac7s-2"
      },
      "source": [
        "Regression Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KEvc0BEP7vxG"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "models = {\n",
        "    'Linear Regression': (LinearRegression(), {}),\n",
        "    'Ridge': (Ridge(), {'alpha': [0.1, 1, 10]}),\n",
        "    'Lasso': (Lasso(), {'alpha': [0.1, 1, 10]}),\n",
        "    'Decision Tree': (DecisionTreeRegressor(), {'max_depth': [3, 5, 7]}),\n",
        "    'Random Forest': (RandomForestRegressor(), {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]}),\n",
        "    'Gradient Boosting': (GradientBoostingRegressor(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heMTPGqU73O2"
      },
      "source": [
        "Converting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vVf9enJm7-bq"
      },
      "outputs": [],
      "source": [
        "y_train_numeric = y_train.map({'bad': 0, 'so-so': 1, 'great': 2})\n",
        "y_test_numeric = y_test.map({'bad': 0, 'so-so': 1, 'great': 2})\n",
        "\n",
        "best_model = None\n",
        "best_rmse = float('inf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mf0j5QS6vRk"
      },
      "source": [
        "Loop through models to see which model works best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N9SqVJE6pPO",
        "outputId": "6b74f638-03e3-417a-8c2c-54a8cbd3034a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression RMSE: 0.31\n",
            "Ridge RMSE: 0.31\n",
            "Lasso RMSE: 0.33\n",
            "Decision Tree RMSE: 0.31\n",
            "Random Forest RMSE: 0.31\n",
            "Gradient Boosting RMSE: 0.31\n",
            "Best Model: Ridge, RMSE: 0.31\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "for name, (model, param_grid) in models.items():\n",
        "    grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "    grid_search.fit(X_train, y_train_numeric)\n",
        "\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_numeric, y_pred))\n",
        "\n",
        "    print(f\"{name} RMSE: {rmse:.2f}\")\n",
        "\n",
        "    if rmse < best_rmse:\n",
        "        best_rmse = rmse\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best Model: {best_model.__class__.__name__}, RMSE: {best_rmse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7wOR4Q3ZRm"
      },
      "source": [
        "Naive Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3VSOe5f22XG",
        "outputId": "a0c68fab-9ea6-4540-9dae-39d5e8e88bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Regressor RMSE: 0.33\n"
          ]
        }
      ],
      "source": [
        "# Calculating the average rating\n",
        "average_rating = np.mean(y_train_numeric)\n",
        "# Predicting the average rating for all test instances\n",
        "y_pred_naive = np.full_like(y_test_numeric, average_rating, dtype=float)\n",
        "\n",
        "# Calculating RMSE for the naive regressor\n",
        "rmse_naive = np.sqrt(mean_squared_error(y_test_numeric, y_pred_naive))\n",
        "print(f\"Naive Regressor RMSE: {rmse_naive:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qg-8vQ582Ho"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXWmnny83tzt"
      },
      "source": [
        "Binarizing the target column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TMvzG1ti35CJ"
      },
      "outputs": [],
      "source": [
        "y_train_bin = y_train_numeric.round().astype(int)  # Round and convert to integers\n",
        "y_test_bin = y_test_numeric.round().astype(int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvTqE0aX9Etk"
      },
      "source": [
        "Trying classification models to find the best one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Au7HHoRp9KMj"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': (LogisticRegression(), {'C': [0.1, 1, 10]}),\n",
        "    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [3, 5, 7]}),\n",
        "    'Random Forest': (RandomForestClassifier(), {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]}),\n",
        "    'Gradient Boosting': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9WtnMNmL9YbD"
      },
      "outputs": [],
      "source": [
        "best_model = None\n",
        "best_accuracy = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrYQRG7S4SrE"
      },
      "source": [
        "Now again a loop to find the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1sf_rjO3gD8",
        "outputId": "2128d763-76ac-422f-d4e2-13082ba3dde9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/just05me/pandas/PandasVenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=100).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.88\n",
            "Decision Tree Accuracy: 0.88\n",
            "Random Forest Accuracy: 0.88\n",
            "Gradient Boosting Accuracy: 0.88\n",
            "Best Model: DecisionTreeClassifier, Accuracy: 0.88\n"
          ]
        }
      ],
      "source": [
        "for name, (model, param_grid) in models.items():\n",
        "    grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=5)\n",
        "    grid_search.fit(X_train, y_train_bin)  # Use binarized target for classification\n",
        "\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test_bin, y_pred)\n",
        "\n",
        "    print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best Model: {best_model.__class__.__name__}, Accuracy: {best_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7DI9UF49n3-"
      },
      "source": [
        "Calculating Accuracy of Naive Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfI6KuYu4h4x",
        "outputId": "bf1b26c9-2232-46b3-bed8-bdf888f13ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Classifier Accuracy: 0.87\n"
          ]
        }
      ],
      "source": [
        "most_frequent_class = max(set(y_train_bin), key=list(y_train_bin).count)  # Use binarized y_train for most frequent class calculation\n",
        "\n",
        "# Predicting the most frequent class for all test instances\n",
        "y_pred_naive = np.full_like(y_test_bin, most_frequent_class, dtype=int)\n",
        "\n",
        "# Calculating accuracy for the naive classifier\n",
        "accuracy_naive = accuracy_score(y_test_bin, y_pred_naive)\n",
        "print(f\"Naive Classifier Accuracy: {accuracy_naive:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsE3IbEd-CMB"
      },
      "source": [
        "Now I am saving it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE2gfcWu-GH5",
        "outputId": "ed2802e7-56ec-4a88-c7af-fa0d22bd76eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['best_model.pkl']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(best_model, 'best_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKI5FOy-FVVl"
      },
      "source": [
        "##Nutrition Facts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpUPOZ8WFXnT"
      },
      "source": [
        "###Collecting Nutrition Facts from USDA API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfA-qtqGFvIV",
        "outputId": "14974224-499e-437b-8bdd-49676f68c3ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique ingredients: 675\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# 1. Load recipes\n",
        "recipes_df = pd.read_csv(\"recipes.csv\")\n",
        "\n",
        "# 2. Add 'ingredients_list' column\n",
        "recipes_df['ingredients_list'] = recipes_df.drop('rating', axis=1).apply(lambda row: ', '.join(row.index[row == 1]), axis=1)\n",
        "\n",
        "# 3. Collect all unique ingredients\n",
        "all_ingredients = set()\n",
        "for ingredients in recipes_df['ingredients_list']:\n",
        "    all_ingredients.update(ingredients.split(', '))\n",
        "unique_ingredients = list(all_ingredients)\n",
        "\n",
        "print(f\"Total unique ingredients: {len(unique_ingredients)}\")\n",
        "\n",
        "# 4. Prepare the API key\n",
        "api_key = 'NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf'\n",
        "\n",
        "# 5. Functions\n",
        "def get_nutrition_data(ingredient):\n",
        "    \"\"\"Fetches nutrition data from the USDA API.\"\"\"\n",
        "    url = f\"https://api.nal.usda.gov/fdc/v1/foods/search?api_key={api_key}&query={ingredient}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if 'foods' in data and data['foods']:\n",
        "            return data['foods'][0]\n",
        "        else:\n",
        "            print(f\"No data found for ingredient: {ingredient}\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data for {ingredient}: {e}\")\n",
        "        return None\n",
        "\n",
        "def transform_to_daily_values(nutrition_data, daily_values_df):\n",
        "    \"\"\"Transforms nutrition values to % daily value.\"\"\"\n",
        "    nutrients = {}\n",
        "    for nutrient_info in nutrition_data.get('foodNutrients', []):\n",
        "        nutrient_name = nutrient_info.get('nutrientName')\n",
        "        nutrient_value = nutrient_info.get('value')\n",
        "\n",
        "        if nutrient_name and nutrient_value is not None:\n",
        "            if nutrient_name.lower() in daily_values_df['nutrient'].str.lower().values:\n",
        "                rda = daily_values_df.loc[daily_values_df['nutrient'].str.lower() == nutrient_name.lower(), 'daily_value'].values[0]\n",
        "                if rda != 0:\n",
        "                    daily_value_percentage = (nutrient_value / rda) * 100\n",
        "                    nutrients[nutrient_name] = round(daily_value_percentage, 2)\n",
        "    return nutrients\n",
        "\n",
        "def sanitize_ingredient(ingredient):\n",
        "    \"\"\"Sanitizes ingredient names.\"\"\"\n",
        "    return ingredient.split('/')[0].strip()\n",
        "\n",
        "# 6. Daily Values Table\n",
        "daily_values_data = [\n",
        "    {'nutrient': 'Vitamin A', 'daily_value': 900},\n",
        "    {'nutrient': 'Vitamin C', 'daily_value': 90},\n",
        "    {'nutrient': 'Calcium', 'daily_value': 1300},\n",
        "    {'nutrient': 'Iron', 'daily_value': 18},\n",
        "    {'nutrient': 'Vitamin D', 'daily_value': 20},\n",
        "    {'nutrient': 'Vitamin E', 'daily_value': 15},\n",
        "    {'nutrient': 'Vitamin K', 'daily_value': 120},\n",
        "    {'nutrient': 'Thiamin', 'daily_value': 1.2},\n",
        "    {'nutrient': 'Riboflavin', 'daily_value': 1.3},\n",
        "    {'nutrient': 'Niacin', 'daily_value': 16},\n",
        "    {'nutrient': 'Vitamin B6', 'daily_value': 1.7},\n",
        "    {'nutrient': 'Folate', 'daily_value': 400},\n",
        "    {'nutrient': 'Vitamin B12', 'daily_value': 2.4},\n",
        "    {'nutrient': 'Biotin', 'daily_value': 30},\n",
        "    {'nutrient': 'Pantothenic Acid', 'daily_value': 5},\n",
        "    {'nutrient': 'Phosphorus', 'daily_value': 1250},\n",
        "    {'nutrient': 'Iodine', 'daily_value': 150},\n",
        "    {'nutrient': 'Magnesium', 'daily_value': 420},\n",
        "    {'nutrient': 'Zinc', 'daily_value': 11},\n",
        "    {'nutrient': 'Selenium', 'daily_value': 55},\n",
        "    {'nutrient': 'Copper', 'daily_value': 0.9},\n",
        "    {'nutrient': 'Manganese', 'daily_value': 2.3},\n",
        "    {'nutrient': 'Chromium', 'daily_value': 35},\n",
        "    {'nutrient': 'Molybdenum', 'daily_value': 45},\n",
        "    {'nutrient': 'Chloride', 'daily_value': 2300},\n",
        "    {'nutrient': 'Potassium', 'daily_value': 4700},\n",
        "    {'nutrient': 'Choline', 'daily_value': 550},\n",
        "    {'nutrient': 'Total Lipid (Fat)', 'daily_value': 65},\n",
        "    {'nutrient': 'Saturated Fat', 'daily_value': 20},\n",
        "    {'nutrient': 'Cholesterol', 'daily_value': 300},\n",
        "    {'nutrient': 'Carbohydrate', 'daily_value': 300},\n",
        "    {'nutrient': 'Sodium', 'daily_value': 2400},\n",
        "    {'nutrient': 'Fiber', 'daily_value': 28},\n",
        "    {'nutrient': 'Protein', 'daily_value': 50},\n",
        "    {'nutrient': 'Sugars', 'daily_value': 50},\n",
        "]\n",
        "daily_values_df = pd.DataFrame(daily_values_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2XW0ERKVyEZ"
      },
      "source": [
        " Fetch and save nutrition data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5KBT_VRQyMt",
        "outputId": "91d6cfbb-c96c-46aa-8522-b836aec9625f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for ingredient: harpercollins\n",
            "Error fetching data for kentucky: HTTPSConnectionPool(host='api.nal.usda.gov', port=443): Max retries exceeded with url: /fdc/v1/foods/search?api_key=NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf&query=kentucky (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdd9dedec00>: Failed to resolve 'api.nal.usda.gov' ([Errno -3] Temporary failure in name resolution)\"))\n",
            "No data found for ingredient: sauté\n",
            "No data found for ingredient: ramadan\n",
            "Error fetching data for beef tenderloin: HTTPSConnectionPool(host='api.nal.usda.gov', port=443): Max retries exceeded with url: /fdc/v1/foods/search?api_key=NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf&query=beef%20tenderloin (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdd9de60380>: Failed to resolve 'api.nal.usda.gov' ([Errno -3] Temporary failure in name resolution)\"))\n",
            "No data found for ingredient: windsor\n",
            "Error fetching data for grand marnier: HTTPSConnectionPool(host='api.nal.usda.gov', port=443): Max retries exceeded with url: /fdc/v1/foods/search?api_key=NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf&query=grand%20marnier (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdd9de62810>: Failed to resolve 'api.nal.usda.gov' ([Errno -3] Temporary failure in name resolution)\"))\n",
            "No data found for ingredient: frankenrecipe\n",
            "Error fetching data for sour cream: HTTPSConnectionPool(host='api.nal.usda.gov', port=443): Max retries exceeded with url: /fdc/v1/foods/search?api_key=NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf&query=sour%20cream (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdd9d1919a0>: Failed to resolve 'api.nal.usda.gov' ([Errno -3] Temporary failure in name resolution)\"))\n",
            "Error fetching data for peach: HTTPSConnectionPool(host='api.nal.usda.gov', port=443): Max retries exceeded with url: /fdc/v1/foods/search?api_key=NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf&query=peach (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdd9d191d60>: Failed to resolve 'api.nal.usda.gov' ([Errno -3] Temporary failure in name resolution)\"))\n",
            "No data found for ingredient: westwood\n",
            "No data found for ingredient: marscarpone\n",
            "No data found for ingredient: leftovers\n",
            "Error fetching data for freeze: HTTPSConnectionPool(host='api.nal.usda.gov', port=443): Max retries exceeded with url: /fdc/v1/foods/search?api_key=NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf&query=freeze (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdd9d190fe0>: Failed to resolve 'api.nal.usda.gov' ([Errno -3] Temporary failure in name resolution)\"))\n",
            "Error fetching data for appetizer: HTTPSConnectionPool(host='api.nal.usda.gov', port=443): Max retries exceeded with url: /fdc/v1/foods/search?api_key=NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf&query=appetizer (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdd9de634a0>: Failed to resolve 'api.nal.usda.gov' ([Errno -3] Temporary failure in name resolution)\"))\n",
            "No data found for ingredient: sukkot\n",
            "No data found for ingredient: yonkers\n",
            "Error fetching data for shavuot: HTTPSConnectionPool(host='api.nal.usda.gov', port=443): Max retries exceeded with url: /fdc/v1/foods/search?api_key=NlGP9zCpXfkf8bS5TXd5gXzfbhzE13wDrImUqNAf&query=shavuot (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdd9d1906b0>: Failed to resolve 'api.nal.usda.gov' ([Errno -3] Temporary failure in name resolution)\"))\n",
            "No data found for ingredient: pescatarian\n",
            "No data found for ingredient: potluck\n",
            "No data found for ingredient: jícama\n",
            "No data found for ingredient: mandoline\n",
            "No data found for ingredient: campari\n",
            "No data found for ingredient: no problem\n",
            "No data found for ingredient: healdsburg\n",
            "No data found for ingredient: weelicious\n",
            "No data found for ingredient: rosh hashanah\n",
            "No data found for ingredient: kwanzaa\n",
            "No data found for ingredient: cookbooks\n",
            "No data found for ingredient: rosé\n",
            "No data found for ingredient: grappa\n",
            "No data found for ingredient: kahlúa\n",
            "✅ Nutrition facts collected and saved to 'nutrition_facts.csv'.\n"
          ]
        }
      ],
      "source": [
        "nutrition_data_list = []\n",
        "processed_ingredients = [sanitize_ingredient(ing) for ing in unique_ingredients]\n",
        "\n",
        "for ingredient in processed_ingredients:\n",
        "    food_data = get_nutrition_data(ingredient)\n",
        "    if food_data:\n",
        "        daily_values = transform_to_daily_values(food_data, daily_values_df)\n",
        "        if daily_values:  # Only add if there is at least some nutrient information\n",
        "            daily_values['Ingredient'] = ingredient\n",
        "            nutrition_data_list.append(daily_values)\n",
        "    time.sleep(1)  # Avoid API rate limits\n",
        "\n",
        "# 8. Save nutrition facts to CSV\n",
        "nutrition_df = pd.DataFrame(nutrition_data_list)\n",
        "nutrition_df.to_csv(\"nutrition_facts.csv\", index=False)\n",
        "\n",
        "print(\"✅ Nutrition facts collected and saved to 'nutrition_facts.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UjhZkbKsO4jd",
        "outputId": "be882260-854e-43d8-d0bf-45986906bffd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein</th>\n",
              "      <th>Total lipid (fat)</th>\n",
              "      <th>Cholesterol</th>\n",
              "      <th>Ingredient</th>\n",
              "      <th>Thiamin</th>\n",
              "      <th>Riboflavin</th>\n",
              "      <th>Niacin</th>\n",
              "      <th>Pantothenic acid</th>\n",
              "      <th>Biotin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11.32</td>\n",
              "      <td>5.80</td>\n",
              "      <td>1.67</td>\n",
              "      <td>tuna</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>47.60</td>\n",
              "      <td>1.83</td>\n",
              "      <td>25.67</td>\n",
              "      <td>buffet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>iced coffee</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>white wine</td>\n",
              "      <td>0.42</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.68</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26.48</td>\n",
              "      <td>1.88</td>\n",
              "      <td>39.67</td>\n",
              "      <td>cocktail</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1.92</td>\n",
              "      <td>11.29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Protein  Total lipid (fat)  Cholesterol   Ingredient  Thiamin  Riboflavin  \\\n",
              "0    11.32               5.80         1.67         tuna      NaN         NaN   \n",
              "1    47.60               1.83        25.67       buffet      NaN         NaN   \n",
              "2     0.00               0.00         0.00  iced coffee      NaN         NaN   \n",
              "3     0.14               0.00         0.00   white wine     0.42        1.15   \n",
              "4    26.48               1.88        39.67     cocktail     1.75        1.92   \n",
              "\n",
              "   Niacin  Pantothenic acid  Biotin  \n",
              "0     NaN               NaN     NaN  \n",
              "1     NaN               NaN     NaN  \n",
              "2     NaN               NaN     NaN  \n",
              "3    0.68               NaN     NaN  \n",
              "4   11.29               NaN     NaN  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"nutrition_facts.csv\")\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJbrFNXNJMPs"
      },
      "source": [
        "## Similar Recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR8CurPmJR7j",
        "outputId": "ae23cdc3-d8e2-4652-bdce-a0b349280950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting beautifulsoup4\n",
            "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
            "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /home/just05me/pandas/PandasVenv/lib/python3.12/site-packages (from beautifulsoup4) (4.14.1)\n",
            "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: soupsieve, beautifulsoup4\n",
            "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGhijpOpRtK5",
        "outputId": "55dca1da-fa9b-4cc9-a745-db80d76b6333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error searching for berry, bon appétit, condiment/spread, mint, no-cook, sauce, spring, strawberry, summer: HTTPSConnectionPool(host='www.epicurious.com', port=443): Read timed out. (read timeout=10)\n",
            "Error searching for bon appétit, braise, cinnamon, dairy free, dinner, garlic, lamb shank, low carb, nutmeg, peanut free, red wine, soy free, sugar conscious, tree nut free, vegetable, wheat/gluten-free, winter: HTTPSConnectionPool(host='www.epicurious.com', port=443): Read timed out. (read timeout=10)\n",
            "Error searching for broccoli, noodle, peanut butter, quick & easy, soy, stir-fry, tofu, vegetable: HTTPSConnectionPool(host='www.epicurious.com', port=443): Read timed out. (read timeout=10)\n",
            "Error searching for cheese, dinner, eggplant, fish, lunch, noodle, pasta, tomato, vegetable, winter: HTTPSConnectionPool(host='www.epicurious.com', port=443): Read timed out. (read timeout=10)\n",
            "Error searching for beef, carrot, gourmet, onion, potato, soup/stew, soy sauce, stew: HTTPSConnectionPool(host='www.epicurious.com', port=443): Read timed out. (read timeout=10)\n",
            "Error searching for cauliflower, fall, gourmet, parmesan, pasta, simmer, soup/stew, vegetable: HTTPSConnectionPool(host='www.epicurious.com', port=443): Read timed out. (read timeout=10)\n",
            "Error searching for bake, bon appétit, carrot, goat cheese, healthy, kid-friendly, kosher, onion, parsnip, peanut free, pescatarian, potato, side, soy free, sweet potato/yam, tree nut free, vegetarian: HTTPSConnectionPool(host='www.epicurious.com', port=443): Read timed out. (read timeout=10)\n",
            "Error searching for bake, christmas eve, dairy, easter, family reunion, gourmet, kid-friendly, kosher, leek, milk/cream, no sugar added, peanut free, pescatarian, potato, root vegetable, side, soy free, tree nut free, vegetarian, wheat/gluten-free: HTTPSConnectionPool(host='www.epicurious.com', port=443): Read timed out. (read timeout=10)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Parallelization\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=\u001b[32m10\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:  \u001b[38;5;66;03m# Adjust max_workers as needed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     recipes_df[\u001b[33m\"\u001b[39m\u001b[33mepicurious_url\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_epicurious_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipes_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Filtering out recipes without URLs\u001b[39;00m\n\u001b[32m    124\u001b[39m recipes_with_urls = recipes_df.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mepicurious_url\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# import requests\n",
        "# from requests.exceptions import RequestException\n",
        "# from bs4 import BeautifulSoup\n",
        "# import pandas as pd\n",
        "# import time\n",
        "# from concurrent.futures import ThreadPoolExecutor\n",
        "# import joblib  # For caching\n",
        "\n",
        "# # Load recipes\n",
        "# recipes_df = pd.read_csv(\"recipes.csv\")\n",
        "\n",
        "# # Create a \"title\" column from ingredients:\n",
        "# recipes_df[\"title\"] = recipes_df.apply(lambda row: ', '.join(row.index[row == 1]), axis=1)\n",
        "\n",
        "# # Load or initialize URL cache\n",
        "# try:\n",
        "#     url_cache = joblib.load(\"url_cache.pkl\")\n",
        "# except FileNotFoundError:\n",
        "#     url_cache = {}\n",
        "\n",
        "# def get_epicurious_url(recipe_title):\n",
        "#     \"\"\"Searches Epicurious for the recipe and returns the URL.\"\"\"\n",
        "#     if recipe_title in url_cache:\n",
        "#         return url_cache[recipe_title]\n",
        "\n",
        "#     search_url = f\"https://www.epicurious.com/search?content=recipe&query={recipe_title}\"\n",
        "#     headers = {\n",
        "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36'\n",
        "#     }\n",
        "#     retries = 3  # Number of retries\n",
        "#     for _ in range(retries):\n",
        "#         try:\n",
        "#             response = requests.get(search_url, headers=headers, timeout=10)  # Timeout added\n",
        "#             response.raise_for_status()\n",
        "#             soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "#             # Try to find a more specific selector if possible\n",
        "#             first_result = soup.select_one(\"a.result-heading-link\")  # Example\n",
        "#             if first_result:\n",
        "#                 url = first_result[\"href\"]\n",
        "#                 url_cache[recipe_title] = url  # Cache the URL\n",
        "#                 return url\n",
        "#             else:\n",
        "#                 return None  # No results found\n",
        "#         except RequestException as e:\n",
        "#             print(f\"Error searching for {recipe_title}: {e}\")\n",
        "#             if isinstance(e, requests.exceptions.SSLError):\n",
        "#                 print(f\"SSL Error (ignoring verification failed): {e}\")\n",
        "#             time.sleep(2)  # Wait before retrying\n",
        "#     return None  # Return None if all retries fail\n",
        "\n",
        "# # Parallelization\n",
        "# with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust max_workers as needed\n",
        "#     recipes_df[\"epicurious_url\"] = list(executor.map(get_epicurious_url, recipes_df[\"title\"]))\n",
        "\n",
        "# # Filtering out recipes without URLs\n",
        "# recipes_with_urls = recipes_df.dropna(subset=[\"epicurious_url\"])\n",
        "\n",
        "# # Save the results and the cache\n",
        "# recipes_with_urls.to_csv(\"recipes_with_urls.csv\", index=False)\n",
        "# joblib.dump(url_cache, \"url_cache.pkl\")  # Save the cache\n",
        "\n",
        "# print(\"✅ Recipes with URLs saved to 'recipes_with_urls.csv'.\")\n",
        "\n",
        "\n",
        "import requests\n",
        "from requests.exceptions import RequestException\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import joblib  # For caching\n",
        "\n",
        "# Load recipes\n",
        "try:\n",
        "    recipes_df = pd.read_csv(\"recipes.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'recipes.csv' not found.\")\n",
        "    exit(1)\n",
        "\n",
        "# Create a \"title\" column from ingredients:\n",
        "recipes_df[\"title\"] = recipes_df.apply(lambda row: ', '.join(row.index[row == 1]), axis=1)\n",
        "\n",
        "# Load or initialize URL cache\n",
        "try:\n",
        "    url_cache = joblib.load(\"url_cache.pkl\")\n",
        "except FileNotFoundError:\n",
        "    url_cache = {}\n",
        "\n",
        "def get_epicurious_url(recipe_title):\n",
        "    \"\"\"Searches Epicurious for the recipe and returns the URL.\"\"\"\n",
        "    if recipe_title in url_cache:\n",
        "        return url_cache[recipe_title]\n",
        "\n",
        "    search_url = f\"https://www.epicurious.com/search?content=recipe&query={recipe_title}\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36'\n",
        "    }\n",
        "    retries = 3  # Number of retries\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            response = requests.get(search_url, headers=headers, timeout=10)  # Timeout added\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            # Try to find a more specific selector if possible\n",
        "            first_result = soup.select_one(\"a.result-heading-link\")  # Example\n",
        "            if first_result:\n",
        "                url = first_result[\"href\"]\n",
        "                url_cache[recipe_title] = url  # Cache the URL\n",
        "                return url\n",
        "            else:\n",
        "                return None  # No results found\n",
        "        except RequestException as e:\n",
        "            print(f\"Error searching for {recipe_title}: {e}\")\n",
        "            if isinstance(e, requests.exceptions.SSLError):\n",
        "                print(f\"SSL Error (ignoring verification failed): {e}\")\n",
        "            time.sleep(2)  # Wait before retrying\n",
        "    return None  # Return None if all retries fail\n",
        "\n",
        "# Parallelization\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust max_workers as needed\n",
        "    recipes_df[\"epicurious_url\"] = list(executor.map(get_epicurious_url, recipes_df[\"title\"]))\n",
        "\n",
        "# Filtering out recipes without URLs\n",
        "recipes_with_urls = recipes_df.dropna(subset=[\"epicurious_url\"])\n",
        "\n",
        "# Save the results and the cache\n",
        "recipes_with_urls.to_csv(\"recipes_with_urls.csv\", index=False)\n",
        "joblib.dump(url_cache, \"url_cache.pkl\")  # Save the cache\n",
        "\n",
        "print(\"✅ Recipes with URLs saved to 'recipes_with_urls.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbouASxzR9-E"
      },
      "source": [
        "filtering out recipes without urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tpZpXWn7SB3Y"
      },
      "outputs": [],
      "source": [
        "recipes_with_urls = recipes_df.dropna(subset=[\"epicurious_url\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRhbQYnsSFvf",
        "outputId": "121a97b4-993f-4452-bfea-3b9d0ed16f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Recipes with URLs saved to 'recipes_with_urls.csv'.\n"
          ]
        }
      ],
      "source": [
        "recipes_with_urls.to_csv(\"recipes_with_urls.csv\", index=False)\n",
        "print(\"✅ Recipes with URLs saved to 'recipes_with_urls.csv'.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PandasVenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
